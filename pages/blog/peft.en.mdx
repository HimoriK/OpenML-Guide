---
title: "In-depth Fine Tuning: PEFT with LoRA & QLoRA"
date: Jun 1, 2023
description: In neural networks, the weight matrices consist of floating-point numbers, typically stored in the 32-bit floating-point data type.
---

import Image from "next/image";

<div className="flex flex-col items-center gap-4">

# In-depth Fine Tuning: PEFT with LoRA & QLoRA

  <small>[Severus](https://aidenybai.com) JUN 1 2023</small>
</div>

---

##

<div style={{ position: "relative" }}>
  <Image
    src="/static/img/blog-img/qlora&lora.png"
    alt="neural netowrk img"
    height="300"
    width="1000"
    priority
  />
</div>

Fine-tuning is a crucial step in unlocking the full potential of large language models (LLMs). It involves adapting the pre-trained LLM to a specific task or domain, resulting in significantly improved performance on that task. However, complete fine-tuning can be computationally expensive and memory-intensive, limiting their applicability on resource-constrained platforms.

To address this challenge, researchers have developed [parameter-efficient fine-tuning](https://huggingface.co/docs/peft/index) (PEFT) techniques. These techniques aim to achieve similar or even better performance than traditional fine-tuning while significantly reducing the number of parameters and memory footprint. Among the most promising PEFT approaches are [LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora) (Low-Rank Adaptation) and [QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) (Quantized LoRA).

<br />
<div style={{ position: "relative" }}>
  <Image
    src="/static/img/blog-img/weights.png"
    alt="neural netowrk img"
    height="300"
    width="1000"
    priority
  />
</div>

In neural networks, the weight matrices consist of [floating-point numbers](https://en.wikipedia.org/wiki/Floating-point_arithmetic), typically stored in the 32-bit floating-point data type. Computers internally represent these floating-point values using binary code, achieved by flipping bits for the [sign](https://en.wikipedia.org/wiki/Integer_(computer_science)), [exponent](https://en.wikipedia.org/wiki/Exponentiation), and [significant](https://en.wikipedia.org/wiki/Significand) (mantissa). This binary representation is fundamental to accurately handle and process numerical data within the neural network architecture.

To reduce precision, a common approach is adjusting the data types. One option is shifting to [half precision](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) (16-bit), utilizing only half the bits for number representation, cutting memory needs in half. However, there's a trade-off, where precision is sacrificed.

In the following example, we've the value $1 - 2^{-24} \approxeq 0.99999994$ in a 32-bit system. Upon reducing the precision by half, the resulting value becomes 0.99951172. This adjustment illustrates the impact of precision reduction on numerical representation.
<br />
<div style={{ position: "relative" }}>
  <Image
    src="/static/img/blog-img/floating_points.png"
    alt="neural netowrk img"
    height="300"
    width="1000"
    priority
  />
</div>

In the context of [single-precision](https://en.wikipedia.org/wiki/Single-precision_floating-point_format), 32-bit format, a single bit is allocated to denote the sign of the number, either positive or negative. Eight bits are dedicated to the exponent, which, given its binary nature, represents 2 raised to a specific power. The remaining 23 bits serve to encode the digits composing the number, referred to as the significant. In half-precision further reduces the allocated space, employing merely five bits for the exponent and ten for the significant.

## Parameter Efficient Fine Tuning (PEFT) 

Modern LLMs such as Gemini or GPT-4 have demonstrated impressive capabilities. However, training these models requires hundreds of billions of parameters and massive datasets. This can be computationally expensive. A new technique called PEFT promises to unlock more from AI models while using far fewer computational resources.

PEFT focuses on tweaking the top layers of a large pre-trained model. The technique freezes the initial model weights, then fine-tunes only the later layers on a specific new dataset or task. This selective re-training reduces compute needed by 100x or more versus full re-training, but still tailors models to new domains.

In tests, PEFT matched or even exceeded the performance of models with 10x more parameters that were trained conventionally from scratch. Researchers were able to take a 6 billion parameter "teacher" model and create specialized 1.3 billion parameter "student" models that outperformed the generalist teacher.

Fine-tuning parameters efficiently can be done in different ways. Two popular and effective methods are Low Rank Parameter (LoRA) and Quantized Low Rank Parameter (QLoRA).

## Low-Rank Parameters (LoRA) 

LoRA is applied to the internal weight matrices that store the "knowledge" within neural networks. These matrices multiply input vectors to produce outputs. Conventional tuning alters all elements freely. But LoRA constraints matrices to low-rank approximations.

Mathematically, the rank of a matrix defines the dimensions spanned by its columns and rows. Low rank matrices can be expressed more compactly without losing expressive capacity. For example, a diagonal matrix only needs to tune its diagonal rather than all entries.

In practice, LoRA replaces weight matrices with a product of two smaller matrices. Say the original matrix $W$ has shape $m \times n$. LoRA approximates $\Delta W \approx A \times B$, where $A = m \times r$, $B = r \times n$, and $r$ is the new reduced rank [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)). This shrinks total trainable elements from $m \times n$ to $m \times r + r \times n$.
During training, gradient updates adapt $A$ and $B$. Their product implicitly tunes $W$ but with drastically fewer parameters. This regularizes models, reduces overfitting, and multiplies training efficiency. Despite using $98\%$ less memory, LoRA models can even outperform conventional counterparts.

Consider a scenario in which we've a neural network with a [feedforward layer](https://en.wikipedia.org/wiki/Feedforward_neural_network) containing a $725 \times 725$ weight matrix $W$. This has 725 input neurons and 725 output neurons, with a complete cross-connection between them through the matrix multiplying input vectors to produce output vectors.
Initially, $W$ has $725 \times 725 = 525,625$ trainable parameters. We decide to apply LoRA here to improve efficiency. Choosing a LoRA adapter rank ($r$) of $64$, we decompose the original $W$ into two smaller matrices:

$A$ with dimensions $725 \times 64$ and $B$ with dimensions $64 \times 725$

So our decomposition looks like $W \approx A \times B$ $i.e.$ $\Delta W_{A \times B}$

Where $A$ contains 46,400 parameters and $B$ contains 46,400 parameters. Together through their product they still effectively represent $W$ but with far fewer total parameters - $92,800$ vs the original $525,625$. This is an $82.4\%$ reduction in parameters from the full $W$!

We then only update $A$ and $B$ during training rather than directly altering $W$. Updating two matrix factors with $\frac{1}{5}^{\text{th}}$ the number of elements provides computational and statistical efficiency. And we still retain the representational capacity of a full $725 \times 725$ matrix implicitly through the matrix product.

So by applying LoRA we reduced parameters in W by $80\%$ while maintaining comparable model capacity. This faster training, smaller memory footprint and reduced risk of overfit allows much wider access to large scale models.

### LoRA finetuning with HuggingFace

For HuggingFace implementation of LoRA finetuning, utilize the [PEFT library](https://pypi.org/project/peft/) to integrate LoRA adapters into the model, employing them as the update matrices.

```py copy
from transformers import AutoModelForCausalLM

# load model from the hub
model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map="auto")

from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType

# Define LoRA Config 
lora_config = LoraConfig(
 r=16, 
 lora_alpha=32,
 target_modules=['query_key_value']
 lora_dropout=0.05,
 bias="none",
 task_type=TaskType.SEQ_2_SEQ_LM
)
# prepare int-8 model for training
model = prepare_model_for_int8_training(model)

# add LoRA adaptor
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817
```

## Quantized Low-Ranking Adaptation (QLoRA)

Quantized Low-Rank Adaptation (QLoRA) tackles the challenge of compression and performance maintenance by merging quantization and low-rank adaptation techniques. Building upon LoRA, QLoRA optimizes efficiency by quantizing the weight values of the base network, transitioning from high-resolution data types, like Float32, to more space-efficient lower-resolution data types, such as int4. This approach results in substantial compression gains while upholding performance standards.

QLoRA stands out as a top-performing PEFT technique due to three essential optimizations:

### 4-Bit Normal Float (NF4)

Quantization reduces the number of bits used to represent data, leading to smaller memory footprints. In QLoRA, pre-trained weights are quantized from the original [32-bit floating-point](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) format (FP32) to a lower-precision format, such as [4-bit](https://en.wikipedia.org/wiki/4-bit_computing) [fixed-point](https://en.wikipedia.org/wiki/Fixed-point_arithmetic) (NF4). This reduces the memory requirement by a factor of 8.

But how does NF4 achieve such feats? The secret lies in its two-step process:

- Normalization: Each weight undergoes a transformation to ensure its values center around zero and have a consistent spread. This ensures that the information within the weights is arranged in a way that facilitates efficient encoding.

- Scaling and Rounding: With the weights normalized, NF4 applies a carefully calibrated scaling factor to fit them within the four-bit range. Similar to a tailor expertly adjusting a garment to fit a specific size, this scaling ensures efficient storage without compromising vital information. Finally, the scaled values are meticulously rounded to the nearest representable value within the four-bit format.

The compression pipeline works in three main steps:

Estimate the $2^b + 1$ ($b$ = number of bits)  quantiles across the $0$ to $1$ distribution to best partition probability mass given k-bit constraints.
Normalize the full weight tensor values into the $[-1, +1]$ range using the transform:
$w_{\text{norm}} = \frac{w}{\max(\lvert w \rvert)}$

w_norm = w / max(abs(w))

Centering on zero expands values to saturate discrete capacity.

Quantize normalized weights w_norm by mapping into the nearest quantile region identified previously. Store as discrete k-bit code based on which of the 2^k + 1 quantiles value falls within.
Decompressing indexes k-bit codes back to their quantile to produce discretized approximated values for computation.

By tailoring quantization to statistically optimized boundaries, Normal Float retains more information per bit compared to arbitrary rounding. Combined with LoRA for parameters reduction, Normal Float acceleration preserves accuracy.

However, NF4 is just the first act in QLoRA's transformative performance. Following the quantization process, Low-Rank Adaptation (LoRA) takes center stage. Imagine dividing the LLM's knowledge into distinct chapters. LoRA skillfully decomposes the weights into a low-rank matrix, a concise summary of the core knowledge, and a set of trainable adaptation parameters. These parameters act as annotations, allowing the LLM to adapt and fine-tune its knowledge to specific tasks without requiring significant alterations to the core structure.


<br />
<div style={{ position: "relative" }}>
  <Image
    src="/static/img/blog-img/qlora.png"
    alt="neural netowrk img"
    height="300"
    width="1000"
    priority
  />
</div>

